{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUwiDnKQmJZb"
   },
   "source": [
    "\n",
    "\n",
    "Yelp Review dataset is quite large with a set of 560,000 highly polar yelp reviews for training. The testing data consists of 38,000 reviews. This is a dataset for binary classification. This dataset is a subset of the yelp dataset that was originally published in the Yelp Dataset challenge in 2015. <br> \n",
    "\n",
    "\n",
    "The Yelp Review Polarity dataset is constructed by categorizing 1 and 2 stars as negative reviews and 3 and 4 stars as positive reviews. We have 280k train positive and 280 train negative reviews and 19k positive test and 19k negative test reviews. <br>\n",
    "\n",
    "Homepage: https://course.fast.ai/datasets<br>\n",
    "\n",
    "Source code: tfds.text.yelp_polarity_reviews\n",
    "\n",
    "Versions:\n",
    "\n",
    "0.2.0 (default): No release notes.\n",
    "Download size: 158.67 MiB\n",
    "\n",
    "Dataset size: 435.14 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFmRN3RI8XED"
   },
   "source": [
    "Since 2017, the Transformer architecture model started to feature more prominently in natural language processing. Using neural attention, it is possible to build powerful sequence models without the need for recurrent or convolution layers. Essentially it takes a sequence as input (often a sentence or paragraph) and translates that into a different sequence. This is the task highly influential for many of the important natural language appplications:\n",
    "\n",
    "\n",
    "*   Text Generation\n",
    "*   Chatbots\n",
    "*   Translations\n",
    "*  Summarization\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfccUEzfmJZe"
   },
   "source": [
    "\n",
    "\n",
    "In short, using self-attention involves scoring the important features of a set of text or an image. Higher score is given to important features and lower scores around the edges or less important features. In natural language processing, self-attention makes word embeddings smarter by representing tokens in the sequence that makes them context-aware. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfJGIx4EmJZj"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnF865oHmJZk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten, Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMmbJHDImJZm",
    "outputId": "f73e5875-0570-42f8-ce30-c53fb27790d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook requires TensorFlow 2.0 or above\n",
      "TensorFlow version:  2.8.2\n",
      "Keras version:  2.8.0\n"
     ]
    }
   ],
   "source": [
    "def version_checker():\n",
    "  \"\"\"The function makes sure we are using the correct version of keras and tensorflow.\"\"\"\n",
    "  print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "  print(\"TensorFlow version: \", tf.__version__)\n",
    "  assert version.parse(tf.__version__).release[0] >=2\n",
    "  print(\"Keras version: \", keras.__version__)\n",
    "version_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-hkaU_TDRDQ",
    "outputId": "ad272324-3bea-44ac-e282-73e3cd5e2f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pl0z_01vmJZn",
    "outputId": "ca2982d6-4217-4715-80f8-19ba25a760f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0604 16:44:04.886303 140429515790208 download_and_prepare.py:200] Running download_and_prepare for dataset(s):\n",
      "yelp_polarity_reviews\n",
      "I0604 16:44:04.886712 140429515790208 dataset_builder.py:811] No config specified, defaulting to first: yelp_polarity_reviews/plain_text\n",
      "I0604 16:44:04.887211 140429515790208 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/yelp_polarity_reviews/plain_text/0.1.0\n",
      "I0604 16:44:04.889339 140429515790208 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/yelp_polarity_reviews/plain_text/0.1.0\n",
      "I0604 16:44:04.890531 140429515790208 download_and_prepare.py:138] download_and_prepare for dataset yelp_polarity_reviews/plain_text/0.1.0...\n",
      "I0604 16:44:04.890706 140429515790208 dataset_builder.py:299] Reusing dataset yelp_polarity_reviews (/root/tensorflow_datasets/yelp_polarity_reviews/plain_text/0.1.0)\n",
      "\u001b[1mname: \"yelp_polarity_reviews\"\n",
      "description: \"Large Yelp Review Dataset.\\nThis is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \\nORIGIN\\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\\nrefer to http://www.yelp.com/dataset\\n\\nThe Yelp reviews polarity dataset is constructed by\\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\\nIt is first used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\\nfor Text Classification. Advances in Neural Information Processing Systems 28\\n(NIPS 2015).\\n\\n\\nDESCRIPTION\\n\\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\\n19,000 testing samples are take randomly. In total there are 560,000 trainig\\nsamples and 38,000 testing samples. Negative polarity is class 1,\\nand positive class 2.\\n\\nThe files train.csv and test.csv contain all the training samples as\\ncomma-sparated values. There are 2 columns in them, corresponding to class\\nindex (1 and 2) and review text. The review texts are escaped using double\\nquotes (\\\"), and any internal double quote is escaped by 2 double quotes (\\\"\\\").\\nNew lines are escaped by a backslash followed with an \\\"n\\\" character,\\nthat is \\\"\\n\\\".\"\n",
      "citation: \"@article{zhangCharacterlevelConvolutionalNetworks2015,\\n  archivePrefix = {arXiv},\\n  eprinttype = {arxiv},\\n  eprint = {1509.01626},\\n  primaryClass = {cs},\\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\\n  journal = {arXiv:1509.01626 [cs]},\\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\\n  month = sep,\\n  year = {2015},\\n}\"\n",
      "location {\n",
      "  urls: \"https://course.fast.ai/datasets\"\n",
      "}\n",
      "schema {\n",
      "  feature {\n",
      "    name: \"label\"\n",
      "    type: INT\n",
      "  }\n",
      "  feature {\n",
      "    name: \"text\"\n",
      "    type: BYTES\n",
      "  }\n",
      "}\n",
      "splits {\n",
      "  name: \"test\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 38000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      type: BYTES\n",
      "      bytes_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 38000\n",
      "  num_bytes: 28899521\n",
      "}\n",
      "splits {\n",
      "  name: \"train\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 560000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      type: BYTES\n",
      "      bytes_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 140000\n",
      "  shard_lengths: 140000\n",
      "  shard_lengths: 140000\n",
      "  shard_lengths: 140000\n",
      "  num_bytes: 427375692\n",
      "}\n",
      "supervised_keys {\n",
      "  input: \"text\"\n",
      "  output: \"label\"\n",
      "}\n",
      "version: \"0.1.0\"\n",
      "download_size: 166373201\n",
      "config_name: \"plain_text\"\n",
      "config_description: \"Plain text\"\n",
      "\u001b[0m\n",
      "W0604 16:44:04.891068 140429515790208 text_feature.py:61] TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "I0604 16:44:04.891276 140429515790208 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/yelp_polarity_reviews/bytes/0.1.0\n",
      "I0604 16:44:04.892735 140429515790208 download_and_prepare.py:138] download_and_prepare for dataset yelp_polarity_reviews/bytes/0.1.0...\n",
      "I0604 16:44:04.892905 140429515790208 dataset_builder.py:299] Reusing dataset yelp_polarity_reviews (/root/tensorflow_datasets/yelp_polarity_reviews/bytes/0.1.0)\n",
      "\u001b[1mname: \"yelp_polarity_reviews\"\n",
      "description: \"Large Yelp Review Dataset.\\nThis is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \\nORIGIN\\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\\nrefer to http://www.yelp.com/dataset\\n\\nThe Yelp reviews polarity dataset is constructed by\\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\\nIt is first used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\\nfor Text Classification. Advances in Neural Information Processing Systems 28\\n(NIPS 2015).\\n\\n\\nDESCRIPTION\\n\\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\\n19,000 testing samples are take randomly. In total there are 560,000 trainig\\nsamples and 38,000 testing samples. Negative polarity is class 1,\\nand positive class 2.\\n\\nThe files train.csv and test.csv contain all the training samples as\\ncomma-sparated values. There are 2 columns in them, corresponding to class\\nindex (1 and 2) and review text. The review texts are escaped using double\\nquotes (\\\"), and any internal double quote is escaped by 2 double quotes (\\\"\\\").\\nNew lines are escaped by a backslash followed with an \\\"n\\\" character,\\nthat is \\\"\\n\\\".\"\n",
      "citation: \"@article{zhangCharacterlevelConvolutionalNetworks2015,\\n  archivePrefix = {arXiv},\\n  eprinttype = {arxiv},\\n  eprint = {1509.01626},\\n  primaryClass = {cs},\\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\\n  journal = {arXiv:1509.01626 [cs]},\\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\\n  month = sep,\\n  year = {2015},\\n}\"\n",
      "location {\n",
      "  urls: \"https://course.fast.ai/datasets\"\n",
      "}\n",
      "schema {\n",
      "  feature {\n",
      "    name: \"label\"\n",
      "    type: INT\n",
      "  }\n",
      "  feature {\n",
      "    name: \"text\"\n",
      "    type: INT\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "splits {\n",
      "  name: \"test\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 38000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        min: 33.0\n",
      "        max: 127.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 38000\n",
      "  num_bytes: 28899521\n",
      "}\n",
      "splits {\n",
      "  name: \"train\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 560000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        min: 33.0\n",
      "        max: 127.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 140000\n",
      "  shard_lengths: 140000\n",
      "  shard_lengths: 140000\n",
      "  shard_lengths: 140000\n",
      "  num_bytes: 427375692\n",
      "}\n",
      "supervised_keys {\n",
      "  input: \"text\"\n",
      "  output: \"label\"\n",
      "}\n",
      "version: \"0.1.0\"\n",
      "download_size: 166373201\n",
      "config_name: \"bytes\"\n",
      "config_description: \"Uses byte-level text encoding with `tfds.deprecated.text.ByteTextEncoder`\"\n",
      "\u001b[0m\n",
      "W0604 16:44:04.893368 140429515790208 text_feature.py:61] TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "I0604 16:44:04.893661 140429515790208 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/yelp_polarity_reviews/subwords8k/0.1.0\n",
      "I0604 16:44:04.934995 140429515790208 download_and_prepare.py:138] download_and_prepare for dataset yelp_polarity_reviews/subwords8k/0.1.0...\n",
      "I0604 16:44:04.935130 140429515790208 dataset_builder.py:299] Reusing dataset yelp_polarity_reviews (/root/tensorflow_datasets/yelp_polarity_reviews/subwords8k/0.1.0)\n",
      "\u001b[1mname: \"yelp_polarity_reviews\"\n",
      "description: \"Large Yelp Review Dataset.\\nThis is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \\nORIGIN\\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\\nrefer to http://www.yelp.com/dataset\\n\\nThe Yelp reviews polarity dataset is constructed by\\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\\nIt is first used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\\nfor Text Classification. Advances in Neural Information Processing Systems 28\\n(NIPS 2015).\\n\\n\\nDESCRIPTION\\n\\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\\n19,000 testing samples are take randomly. In total there are 560,000 trainig\\nsamples and 38,000 testing samples. Negative polarity is class 1,\\nand positive class 2.\\n\\nThe files train.csv and test.csv contain all the training samples as\\ncomma-sparated values. There are 2 columns in them, corresponding to class\\nindex (1 and 2) and review text. The review texts are escaped using double\\nquotes (\\\"), and any internal double quote is escaped by 2 double quotes (\\\"\\\").\\nNew lines are escaped by a backslash followed with an \\\"n\\\" character,\\nthat is \\\"\\n\\\".\"\n",
      "citation: \"@article{zhangCharacterlevelConvolutionalNetworks2015,\\n  archivePrefix = {arXiv},\\n  eprinttype = {arxiv},\\n  eprint = {1509.01626},\\n  primaryClass = {cs},\\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\\n  journal = {arXiv:1509.01626 [cs]},\\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\\n  month = sep,\\n  year = {2015},\\n}\"\n",
      "location {\n",
      "  urls: \"https://course.fast.ai/datasets\"\n",
      "}\n",
      "schema {\n",
      "  feature {\n",
      "    name: \"label\"\n",
      "    type: INT\n",
      "  }\n",
      "  feature {\n",
      "    name: \"text\"\n",
      "    type: INT\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "splits {\n",
      "  name: \"test\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 38000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        min: 1.0\n",
      "        max: 8046.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 38000\n",
      "  num_bytes: 12141727\n",
      "}\n",
      "splits {\n",
      "  name: \"train\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 560000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        min: 1.0\n",
      "        max: 8046.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 280000\n",
      "  shard_lengths: 280000\n",
      "  num_bytes: 179633315\n",
      "}\n",
      "supervised_keys {\n",
      "  input: \"text\"\n",
      "  output: \"label\"\n",
      "}\n",
      "version: \"0.1.0\"\n",
      "download_size: 166373201\n",
      "config_name: \"subwords8k\"\n",
      "config_description: \"Uses `tfds.deprecated.text.SubwordTextEncoder` with 8k vocab size\"\n",
      "\u001b[0m\n",
      "W0604 16:44:04.935452 140429515790208 text_feature.py:61] TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "I0604 16:44:04.935672 140429515790208 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/yelp_polarity_reviews/subwords32k/0.1.0\n",
      "I0604 16:44:05.065589 140429515790208 download_and_prepare.py:138] download_and_prepare for dataset yelp_polarity_reviews/subwords32k/0.1.0...\n",
      "I0604 16:44:05.065776 140429515790208 dataset_builder.py:299] Reusing dataset yelp_polarity_reviews (/root/tensorflow_datasets/yelp_polarity_reviews/subwords32k/0.1.0)\n",
      "\u001b[1mname: \"yelp_polarity_reviews\"\n",
      "description: \"Large Yelp Review Dataset.\\nThis is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \\nORIGIN\\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\\nrefer to http://www.yelp.com/dataset\\n\\nThe Yelp reviews polarity dataset is constructed by\\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\\nIt is first used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\\nfor Text Classification. Advances in Neural Information Processing Systems 28\\n(NIPS 2015).\\n\\n\\nDESCRIPTION\\n\\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\\n19,000 testing samples are take randomly. In total there are 560,000 trainig\\nsamples and 38,000 testing samples. Negative polarity is class 1,\\nand positive class 2.\\n\\nThe files train.csv and test.csv contain all the training samples as\\ncomma-sparated values. There are 2 columns in them, corresponding to class\\nindex (1 and 2) and review text. The review texts are escaped using double\\nquotes (\\\"), and any internal double quote is escaped by 2 double quotes (\\\"\\\").\\nNew lines are escaped by a backslash followed with an \\\"n\\\" character,\\nthat is \\\"\\n\\\".\"\n",
      "citation: \"@article{zhangCharacterlevelConvolutionalNetworks2015,\\n  archivePrefix = {arXiv},\\n  eprinttype = {arxiv},\\n  eprint = {1509.01626},\\n  primaryClass = {cs},\\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\\n  journal = {arXiv:1509.01626 [cs]},\\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\\n  month = sep,\\n  year = {2015},\\n}\"\n",
      "location {\n",
      "  urls: \"https://course.fast.ai/datasets\"\n",
      "}\n",
      "schema {\n",
      "  feature {\n",
      "    name: \"label\"\n",
      "    type: INT\n",
      "  }\n",
      "  feature {\n",
      "    name: \"text\"\n",
      "    type: INT\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "splits {\n",
      "  name: \"test\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 38000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 38000\n",
      "        }\n",
      "        min: 1.0\n",
      "        max: 32635.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 38000\n",
      "  num_bytes: 11529820\n",
      "}\n",
      "splits {\n",
      "  name: \"train\"\n",
      "  num_shards: 1\n",
      "  statistics {\n",
      "    num_examples: 560000\n",
      "    features {\n",
      "      name: \"label\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        max: 1.0\n",
      "      }\n",
      "    }\n",
      "    features {\n",
      "      name: \"text\"\n",
      "      num_stats {\n",
      "        common_stats {\n",
      "          num_non_missing: 560000\n",
      "        }\n",
      "        min: 1.0\n",
      "        max: 32635.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  shard_lengths: 280000\n",
      "  shard_lengths: 280000\n",
      "  num_bytes: 170556321\n",
      "}\n",
      "supervised_keys {\n",
      "  input: \"text\"\n",
      "  output: \"label\"\n",
      "}\n",
      "version: \"0.1.0\"\n",
      "download_size: 166373201\n",
      "config_name: \"subwords32k\"\n",
      "config_description: \"Uses `tfds.deprecated.text.SubwordTextEncoder` with 32k vocab size\"\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# register  yelp_polarity_reviews so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=yelp_polarity_reviews\n",
    "\n",
    "dataset, info = tfds.load('yelp_polarity_reviews', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],batch_size = 32, as_supervised=True)\n",
    "train_ds, val_ds, test_ds = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daxNxbjwxrKI"
   },
   "source": [
    "During the vectorization, I selected the max_token argument to be 300 which means that the maximum size of the vocabulary is 300. \\\n",
    "Secondly, i select the output sequence length to be 3000, which means that the output has time dimension padded to 3000. \\\n",
    "Thirdly, I am outputting integers so the vectorization step outputs integer indices - one integer index per token.\\\n",
    "Finally, I am doing preprocessing by converting all text to lowercase and stripping any punctuations from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-DVBPWGmJZp"
   },
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "max_tokens = 3000\n",
    "lower_no_punc = \"lower_and_strip_punctuation\"\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    standardize =lower_no_punc,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPVABV8vmJZq"
   },
   "source": [
    "## Transformer Encoder Implemented As Subclassed `Layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxjX9BMSbinO"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "  \"\"\"Representation of a transformer for sentiment classification purposes.\"\"\"\n",
    "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "    \"\"\"Instantiating the Transformer.\n",
    "    Inputs: \n",
    "    embed_dim: size of the input token vectors\n",
    "    dense_dim: size of the hidden dense layer\n",
    "    num_heads: number of attention heads\"\"\"\n",
    "    super().__init__(**kwargs)\n",
    "    self.embed_dim = embed_dim\n",
    "    self.attention = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.dense_proj = keras.Sequential(\n",
    "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "          layers.Dense(embed_dim),]\n",
    "          )\n",
    "\n",
    "\n",
    "  def call(self, inputs, mask=None):\n",
    "    \"\"\"We are defining the forward pass of the model in the call method. This uses the previous layers created in the model.\n",
    "    Instantiate the subclass and call it on the data to create model weights.\"\"\"\n",
    "    if mask is not None:\n",
    "        mask = mask[:, tf.newaxis, :]\n",
    "    attention_output = self.attention(\n",
    "        inputs, inputs, attention_mask=mask)\n",
    "\n",
    "    return self.layernorm_2(proj_output)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config()\n",
    "    config.update({\n",
    "        \"embed_dim\": self.embed_dim,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"dense_dim\": self.dense_dim,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtiaDM_nmJZq"
   },
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/TransformerEncoder.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N2L9IJT_iwh"
   },
   "source": [
    "I have used a vocabulary size of 3000 to make sure we lose less semantics from the text body of each of the reviews.\n",
    "\n",
    "Secondly, I am only dropping 25% of the lowest performing nodes to retain information from 75% of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNKHwoBAmJZr",
    "outputId": "315223e4-15e2-437e-fc03-b2094b87a227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         768000    \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 256)        543776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,312,290\n",
      "Trainable params: 1,312,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "outputs = layers.Dense(2, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxd-c_eXGvMw"
   },
   "source": [
    "We are applying two callbacks in the fit method using the callbacks arg. It takes a list of callbacks. First callback is the model checkpoint that save the current weights after every epoch. Second callbacks is the early stopping. It monitors the model's validation accuracy and interrupts training when accuracy has stopped improving for three epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJP3RBaBmJZr",
    "outputId": "2b0a98d8-0f24-472f-eb52-445b79f33b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16625/16625 [==============================] - 435s 26ms/step - loss: 0.2266 - accuracy: 0.9072 - val_loss: 0.2129 - val_accuracy: 0.9165\n",
      "Epoch 2/200\n",
      "16625/16625 [==============================] - 442s 27ms/step - loss: 0.2024 - accuracy: 0.9185 - val_loss: 0.1996 - val_accuracy: 0.9204\n",
      "Epoch 3/200\n",
      "16625/16625 [==============================] - 444s 27ms/step - loss: 0.1981 - accuracy: 0.9202 - val_loss: 0.1975 - val_accuracy: 0.9199\n",
      "Epoch 4/200\n",
      "16625/16625 [==============================] - 444s 27ms/step - loss: 0.1950 - accuracy: 0.9217 - val_loss: 0.1928 - val_accuracy: 0.9231\n",
      "Epoch 5/200\n",
      "16625/16625 [==============================] - 448s 27ms/step - loss: 0.1935 - accuracy: 0.9226 - val_loss: 0.1928 - val_accuracy: 0.9231\n",
      "Epoch 6/200\n",
      "16625/16625 [==============================] - 447s 27ms/step - loss: 0.1914 - accuracy: 0.9232 - val_loss: 0.1917 - val_accuracy: 0.9241\n",
      "Epoch 7/200\n",
      "16625/16625 [==============================] - 446s 27ms/step - loss: 0.1912 - accuracy: 0.9234 - val_loss: 0.1954 - val_accuracy: 0.9215\n",
      "Epoch 8/200\n",
      "16625/16625 [==============================] - 447s 27ms/step - loss: 0.1902 - accuracy: 0.9241 - val_loss: 0.1953 - val_accuracy: 0.9205\n",
      "Epoch 9/200\n",
      "16625/16625 [==============================] - 447s 27ms/step - loss: 0.1895 - accuracy: 0.9244 - val_loss: 0.1935 - val_accuracy: 0.9210\n",
      "1188/1188 [==============================] - 13s 11ms/step - loss: 0.1855 - accuracy: 0.9253\n",
      "Test acc: 0.925\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "    ]\n",
    "                                      \n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp365kdwANRp"
   },
   "source": [
    "Without positional encoding, we seem to get a test accuracy of 92.5% which is the highest test accuracy I have ever gotten in any NLP model ever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ5-2z1WmJZs"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeVw84ygmJZt",
    "outputId": "3b95581b-fd26-4afb-b798-15d9148d37c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posit  (None, None, 256)        781312    \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,326,116\n",
      "Trainable params: 1,326,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "16625/16625 [==============================] - 454s 27ms/step - loss: 0.2203 - accuracy: 0.9110 - val_loss: 0.2073 - val_accuracy: 0.9144\n",
      "Epoch 2/200\n",
      "16625/16625 [==============================] - 454s 27ms/step - loss: 0.2054 - accuracy: 0.9172 - val_loss: 0.1996 - val_accuracy: 0.9202\n",
      "Epoch 3/200\n",
      "16625/16625 [==============================] - 455s 27ms/step - loss: 0.2060 - accuracy: 0.9172 - val_loss: 0.2067 - val_accuracy: 0.9190\n",
      "Epoch 4/200\n",
      "16625/16625 [==============================] - 456s 27ms/step - loss: 0.2017 - accuracy: 0.9190 - val_loss: 0.1972 - val_accuracy: 0.9217\n",
      "Epoch 5/200\n",
      "16625/16625 [==============================] - 456s 27ms/step - loss: 0.1992 - accuracy: 0.9199 - val_loss: 0.1963 - val_accuracy: 0.9222\n",
      "Epoch 6/200\n",
      "16625/16625 [==============================] - 457s 27ms/step - loss: 0.1975 - accuracy: 0.9209 - val_loss: 0.1991 - val_accuracy: 0.9207\n",
      "Epoch 7/200\n",
      "16625/16625 [==============================] - 455s 27ms/step - loss: 0.1983 - accuracy: 0.9203 - val_loss: 0.2028 - val_accuracy: 0.9195\n",
      "Epoch 8/200\n",
      "16625/16625 [==============================] - 456s 27ms/step - loss: 0.1975 - accuracy: 0.9207 - val_loss: 0.2039 - val_accuracy: 0.9180\n",
      "1188/1188 [==============================] - 14s 11ms/step - loss: 0.1907 - accuracy: 0.9249\n",
      "Test acc: 0.925\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJMlN8-yAcGZ"
   },
   "source": [
    "With 3000 tokens and positional embedding, I got a test accuracy of 92.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylLaMZAQzQky",
    "outputId": "51bdcba5-0528-4cfb-c2b3-645af9078118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posit  (None, None, 256)        269312    \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,116\n",
      "Trainable params: 814,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "16625/16625 [==============================] - 457s 27ms/step - loss: 0.2598 - accuracy: 0.8921 - val_loss: 0.2436 - val_accuracy: 0.8989\n",
      "Epoch 2/200\n",
      "16625/16625 [==============================] - 461s 28ms/step - loss: 0.2418 - accuracy: 0.8996 - val_loss: 0.2439 - val_accuracy: 0.8993\n",
      "Epoch 3/200\n",
      "16625/16625 [==============================] - 461s 28ms/step - loss: 0.2387 - accuracy: 0.9008 - val_loss: 0.2404 - val_accuracy: 0.9005\n",
      "Epoch 4/200\n",
      "16625/16625 [==============================] - 462s 28ms/step - loss: 0.2371 - accuracy: 0.9022 - val_loss: 0.2401 - val_accuracy: 0.9005\n",
      "Epoch 5/200\n",
      "16625/16625 [==============================] - 460s 28ms/step - loss: 0.2369 - accuracy: 0.9025 - val_loss: 0.2365 - val_accuracy: 0.9019\n",
      "Epoch 6/200\n",
      "16625/16625 [==============================] - 462s 28ms/step - loss: 0.2369 - accuracy: 0.9026 - val_loss: 0.2389 - val_accuracy: 0.9011\n",
      "Epoch 7/200\n",
      "16625/16625 [==============================] - 462s 28ms/step - loss: 0.2380 - accuracy: 0.9022 - val_loss: 0.2467 - val_accuracy: 0.8984\n",
      "Epoch 8/200\n",
      "16625/16625 [==============================] - 462s 28ms/step - loss: 0.2386 - accuracy: 0.9019 - val_loss: 0.2408 - val_accuracy: 0.8999\n",
      "1188/1188 [==============================] - 14s 12ms/step - loss: 0.2306 - accuracy: 0.9049\n",
      "Test acc: 0.905\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cwOLlMzzQk0"
   },
   "source": [
    "Based on 1000 word vocabulary, the test accuracy reduces from 92.4% to 90.5% which is still respectable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuLRzp1LzYTI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "RS_Final_Yelp_Review_Classification_Transformer-v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
